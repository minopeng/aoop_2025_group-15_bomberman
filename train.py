# train.py
# ç¬¬ä¸‰éšæ®µï¼šç•¢æ¥­è€ƒ (0å¤±èª¤è€å¸« + ä½æ¢ç´¢ç‡)

import os
# å¼·åˆ¶å·¥äººä½¿ç”¨ CPU
os.environ["CUDA_VISIBLE_DEVICES"] = "-1" 

import numpy as np
import multiprocessing as mp
from random import random
from tqdm import tqdm 

from game_board import GameBoard
from ai_player import AIPlayer
from rl_ai_player import RL_AIPlayer

# --- è¨“ç·´è¶…åƒæ•¸ ---
NUM_TOTAL_GAMES = 20000     # å†è·‘ 2 è¬å±€ä¾†æ”¶å°¾
GAMES_PER_BATCH = 32        
TRAIN_THRESHOLD = 512
SAVE_MODEL_EVERY = 50       
REPORT_EVERY = 10           
MODEL_SAVE_PATH = "models/"

# ==========================================
# [æœ€çµ‚ä¿®æ”¹ 1] è€å¸«ç«åŠ›å…¨é–‹ï¼
# 0.0 ä»£è¡¨è€å¸«å®Œå…¨ä¸å¤±èª¤ï¼Œé€™æ˜¯æœ€ç¡¬çš„ä»—
TEACHER_MISTAKE_RATE = 0.0 

# [æœ€çµ‚ä¿®æ”¹ 2] æ”¶æ–‚å¿ƒæ€§
# æ¢ç´¢ç‡é™åˆ° 0.1ï¼Œè®“ AI å°ˆæ³¨æ–¼ã€Œè´æ£‹ã€è€Œä¸æ˜¯ã€Œå˜—è©¦ã€
EPSILON_START = 0.1  
EPSILON_END = 0.01
EPSILON_DECAY = 0.995
# ==========================================

os.makedirs(MODEL_SAVE_PATH, exist_ok=True)

# --- è³‡æ–™å¢å¼· ---
def get_symmetries(board_input, pi_policy):
    state = np.squeeze(board_input)
    pi = pi_policy.reshape(15, 15)
    sym_data = []
    for i in range(4):
        rotated_state = np.rot90(state, i)
        rotated_pi = np.rot90(pi, i)
        sym_data.append((np.expand_dims(rotated_state, 0), rotated_pi.flatten()))
        flipped_state = np.fliplr(rotated_state)
        flipped_pi = np.fliplr(rotated_pi)
        sym_data.append((np.expand_dims(flipped_state, 0), flipped_pi.flatten()))
    return sym_data

# --- å³æ™‚çå‹µè¨ˆç®—æ©Ÿ ---
def calculate_move_quality(board_grid, x, y, color):
    extra_reward = 0
    level = 15
    directions = [(0, 1), (1, 0), (1, 1), (1, -1)]
    for dx, dy in directions:
        count = 1 
        i, j = x + dx, y + dy
        while 0 <= i < level and 0 <= j < level and board_grid[i][j] == color:
            count += 1
            i += dx
            j += dy
        open_end_1 = (0 <= i < level and 0 <= j < level and board_grid[i][j] == 0)
        i, j = x - dx, y - dy
        while 0 <= i < level and 0 <= j < level and board_grid[i][j] == color:
            count += 1
            i -= dx
            j -= dy
        open_end_2 = (0 <= i < level and 0 <= j < level and board_grid[i][j] == 0)
        
        if count == 4 and open_end_1 and open_end_2: extra_reward += 0.5
        elif count == 4 and (open_end_1 or open_end_2): extra_reward += 0.3
        elif count == 3 and open_end_1 and open_end_2: extra_reward += 0.3
        elif count == 2 and open_end_1 and open_end_2: extra_reward += 0.05
    return extra_reward

# --- å·¥äººå‡½å¼ ---
def simulation_worker(args):
    weights, epsilon = args
    
    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
    import tensorflow as tf
    
    board = GameBoard()
    teacher_ai = AIPlayer()
    student_ai = RL_AIPlayer() 
    student_ai.model.set_weights(weights)
    
    # ç¨å¾®å¢åŠ è§€å¯Ÿæ¨¡å¼ï¼Œè®“å­¸ç”Ÿçœ‹è€å¸«å¦‚ä½•å®Œç¾å·¦å³äº’æ
    is_observation_mode = (random() < 0.2) 
    if is_observation_mode:
        p1, p2 = "teacher", "teacher"
    else:
        p1, p2 = "student", "teacher"
        
    current_color = 1
    last_xy = (-1, -1)
    game_over = False
    game_history = []
    winner_color = 0
    
    while not game_over:
        role = p1 if current_color == 1 else p2
        state_tensor = student_ai._prepare_input(board.grid, current_color)
        ax, ay = -1, -1
        
        move_immediate_reward = 0.0 
        
        if role == "student":
            if random() < epsilon:
                ax, ay = student_ai._find_random_empty(board.grid)
            else:
                ax, ay = student_ai.get_move(board.grid, current_color)
            move_immediate_reward = calculate_move_quality(board.grid, ax, ay, current_color)
        else:
            # [è€å¸«é‚è¼¯]
            # é€™ä¸€æ­¥å¾ˆé‡è¦ï¼šTEACHER_MISTAKE_RATE ç¾åœ¨æ˜¯ 0ï¼Œæ‰€ä»¥ make_mistake æ°¸é æ˜¯ False
            make_mistake = (p1 == "student") and (random() < TEACHER_MISTAKE_RATE)
            if make_mistake:
                ax, ay = teacher_ai._find_random_empty(board.grid)
            else:
                ax, ay = teacher_ai.get_move(board.grid, last_xy[0], last_xy[1], current_color)
                if not board.is_empty(ax, ay):
                     ax, ay = teacher_ai._find_random_empty(board.grid)
                 
        target_policy = np.zeros(225)
        target_policy[ax * 15 + ay] = 1.0
        
        game_history.append({
            'state': state_tensor,
            'policy': target_policy,
            'color': current_color,
            'immediate_reward': move_immediate_reward
        })
        
        board.place_stone(ax, ay, current_color)
        last_xy = (ax, ay)
        
        if board.check_win(ax, ay, current_color):
            winner_color = current_color
            game_over = True
        elif board.is_full():
            winner_color = 0
            game_over = True
            
        current_color *= -1
        
    student_played = (p1 == "student")
    return game_history, winner_color, student_played

def train():
    import tensorflow as tf
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError: pass

    # ==========================================
    # [é—œéµä¿®æ”¹ 3] å„ªå…ˆè®€å– latest (å› ç‚ºä½ ä¸Šæ¬¡æ˜¯ä¸­é€”åœæ­¢çš„)
    # ==========================================
    latest_model_path = os.path.join(MODEL_SAVE_PATH, "gomoku_rl_model_latest.keras")
    final_model_path = os.path.join(MODEL_SAVE_PATH, "gomoku_rl_model_final.keras")
    
    model_to_load = None
    if os.path.exists(latest_model_path):
        model_to_load = latest_model_path
    elif os.path.exists(final_model_path):
        model_to_load = final_model_path
        
    if model_to_load:
        print(f"âœ… ç™¼ç¾ç¹¼æ‰¿æ¨¡å‹ï¼š{model_to_load}")
        print(f"ğŸš€ è¼‰å…¥ä¸­... é€²å…¥æœ€çµ‚ç•¢æ¥­è€ƒï¼")
        student_ai = RL_AIPlayer(model_path=model_to_load)
    else:
        print(f"âš ï¸ è­¦å‘Šï¼šæ²’æ‰¾åˆ°ä»»ä½•èˆŠæ¨¡å‹ï¼ä½ ç¢ºå®šè¦å¾é›¶é–‹å§‹æŒ‘æˆ° 0 å¤±èª¤è€å¸«å—ï¼Ÿ(æœƒè¢«è™å¾ˆæ…˜å–”)")
        student_ai = RL_AIPlayer()
    # ==========================================

    num_workers = mp.cpu_count()
    
    print(f"--- å•Ÿå‹•ç¬¬ä¸‰éšæ®µè¨“ç·´ (Teacher Mistake: {TEACHER_MISTAKE_RATE}) ---")
    
    memory_buffer = []
    epsilon = EPSILON_START
    games_completed = 0
    batch_count = 0
    
    recent_student_wins = 0
    recent_teacher_wins = 0
    recent_draws = 0
    recent_games_count = 0
    
    with mp.Pool(processes=num_workers) as pool:
        with tqdm(total=NUM_TOTAL_GAMES, unit="game") as pbar:
            while games_completed < NUM_TOTAL_GAMES:
                current_weights = student_ai.model.get_weights()
                tasks = [(current_weights, epsilon)] * GAMES_PER_BATCH
                
                results = pool.map(simulation_worker, tasks)
                
                batch_games_count = len(results)
                games_completed += batch_games_count
                pbar.update(batch_games_count)
                
                for history, winner, student_played in results:
                    if student_played:
                        recent_games_count += 1
                        if winner == 1: recent_student_wins += 1
                        elif winner == -1: recent_teacher_wins += 1
                        else: recent_draws += 1

                    reward_for_black = 0
                    if winner == 1: reward_for_black = 1.0
                    elif winner == -1: reward_for_black = -1.0
                    else: reward_for_black = -0.1 
                    
                    for step in history:
                        base_val = reward_for_black if step['color'] == 1 else -reward_for_black
                        final_val = base_val + step['immediate_reward']
                        final_val = np.clip(final_val, -1.5, 1.5)
                        
                        aug_data = get_symmetries(step['state'], step['policy'])
                        for s, p in aug_data:
                            memory_buffer.append((s, p, final_val))
                
                if len(memory_buffer) >= TRAIN_THRESHOLD:
                    states = np.vstack([m[0] for m in memory_buffer])
                    policies = np.vstack([m[1] for m in memory_buffer])
                    values = np.array([m[2] for m in memory_buffer])
                    
                    student_ai.model.fit(
                        states, 
                        {'policy_output': policies, 'value_output': values},
                        batch_size=512, 
                        epochs=1, 
                        verbose=0
                    )
                    memory_buffer = []
                    if epsilon > EPSILON_END:
                        epsilon *= EPSILON_DECAY
                
                batch_count += 1
                
                if recent_games_count > 0:
                    win_rate = (recent_student_wins / recent_games_count) * 100
                    pbar.set_postfix({'WR': f"{win_rate:.1f}%", 'Eps': f"{epsilon:.2f}"})

                if batch_count % REPORT_EVERY == 0:
                    if recent_games_count > 0:
                        wr = (recent_student_wins / recent_games_count) * 100
                        tqdm.write(f"\n[æˆ°å ±] è¿‘ {recent_games_count} å ´: å­¸ç”Ÿå‹ {recent_student_wins} | è€å¸«å‹ {recent_teacher_wins} (WR: {wr:.1f}%)")
                        
                        # --- [å¿ƒç†å»ºè¨­] ---
                        if wr > 50:
                            tqdm.write("      ğŸ‘‘ ç¥äº†ï¼å­¸ç”Ÿå·²ç¶“è¶…è¶Šäº†å®Œå…¨é«”çš„è€å¸«ï¼")
                        elif wr > 20:
                            tqdm.write("      âš”ï¸ è¡¨ç¾å„ªç•°ï¼èƒ½å¾ 0 å¤±èª¤è€å¸«æ‰‹ä¸­æ¶ä¸‹é€™éº¼å¤šå‹ï¼Œéå¸¸ä¸å®¹æ˜“ã€‚")
                        elif wr < 5:
                             tqdm.write("      ğŸ›¡ï¸ è‹¦æˆ°ä¸­... è€å¸«é˜²å®ˆå¤ªåš´å¯†äº†ï¼Œé€™æ˜¯æ­£å¸¸çš„ã€‚")
                        # ------------------

                        recent_student_wins = 0
                        recent_teacher_wins = 0
                        recent_draws = 0
                        recent_games_count = 0

                if batch_count % SAVE_MODEL_EVERY == 0:
                    path = os.path.join(MODEL_SAVE_PATH, "gomoku_rl_model_latest.keras")
                    student_ai.save_model(path)

    student_ai.save_model(os.path.join(MODEL_SAVE_PATH, "gomoku_rl_model_final.keras"))
    print("ç•¢æ¥­è€ƒçµæŸï¼æ­å–œä½ çš„ AI å®Œæˆæ‰€æœ‰è¨“ç·´ï¼")

if __name__ == "__main__":
    mp.set_start_method("spawn", force=True)
    train()